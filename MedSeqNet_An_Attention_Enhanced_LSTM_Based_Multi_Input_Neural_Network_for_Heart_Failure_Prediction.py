# -*- coding: utf-8 -*-
"""MedSeqNet: An Attention-Enhanced LSTM-Based Multi-Input Neural Network for Heart Failure Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Vu0ymeBaKYK9JYSvDSO63sZGsClAQ7p
"""

!pip install imbalanced-learn

import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt

from google.colab import files
from collections import Counter
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Concatenate, Flatten

data = pd.read_csv('/content/heart_failure_clinical_records.csv')

duplicates = data[data.duplicated()]

print(f"Number of duplicate records: {len(duplicates)}")
#print(duplicates)

data = data.drop_duplicates()

data.reset_index(drop=True, inplace=True)

data.info()

missing_age_count = data['age'].isnull().sum() + (data['age'] == 0).sum()

print(f"Number of missing values in the 'age' column (including zeros as missing): {missing_age_count}")

data['age'] = data['age'].round()

data['age'] = data['age'].astype(int)

missing_anaemia_count = data['anaemia'].isnull().sum()

print(f"Number of missing values in the 'anaemia' column: {missing_anaemia_count}")

missing_creatinine_phosphokinase_count = data['creatinine_phosphokinase'].isnull().sum() + (data['creatinine_phosphokinase'] == 0).sum()

print(f"Number of missing values in the 'creatinine_phosphokinase' column (including zeros as missing): {missing_creatinine_phosphokinase_count}")

data['creatinine_phosphokinase'] = data['creatinine_phosphokinase'].round()

data['creatinine_phosphokinase'] = data['creatinine_phosphokinase'].astype(int)

missing_diabetes_count = data['diabetes'].isnull().sum()

print(f"Number of missing values in the 'diabetes' column: {missing_diabetes_count}")

missing_ejection_fraction_count = data['ejection_fraction'].isnull().sum() + (data['ejection_fraction'] == 0).sum()

print(f"Number of missing values in the 'ejection_fraction' column (including zeros as missing): {missing_ejection_fraction_count}")

data['ejection_fraction'] = data['ejection_fraction'].round()

data['ejection_fraction'] = data['ejection_fraction'].astype(int)

missing_high_blood_pressure_count = data['high_blood_pressure'].isnull().sum()

print(f"Number of missing values in the 'high_blood_pressure' column: {missing_high_blood_pressure_count}")

missing_platelets_count = data['platelets'].isnull().sum() + (data['platelets'] == 0).sum()

print(f"Number of missing values in the 'eplatelets' column (including zeros as missing): {missing_platelets_count}")

data['platelets'] = data['platelets'].round()

data['platelets'] = data['platelets'].astype(int)

missing_serum_creatinine_count = data['serum_creatinine'].isnull().sum() + (data['serum_creatinine'] == 0).sum()

print(f"Number of missing values in the 'serum_creatinine' column (including zeros as missing): {missing_serum_creatinine_count}")

missing_serum_sodium_count = data['serum_sodium'].isnull().sum() + (data['serum_sodium'] == 0).sum()

print(f"Number of missing values in the 'serum_sodium' column (including zeros as missing): {missing_serum_sodium_count}")

missing_sex_count = data['sex'].isnull().sum()

print(f"Number of missing values in the 'sex' column: {missing_sex_count}")

missing_smoking_count = data['smoking'].isnull().sum()

print(f"Number of missing values in the 'smoking' column: {missing_smoking_count}")

missing_time_count = data['time'].isnull().sum() + (data['time'] == 0).sum()

print(f"Number of missing values in the 'time' column (including zeros as missing): {missing_time_count}")

missing_DEATH_EVENT_count = data['DEATH_EVENT'].isnull().sum()

print(f"Number of missing values in the 'DEATH_EVENT' column (including zeros as missing): {missing_DEATH_EVENT_count}")

data.to_csv('heart_failure_processed.csv', index=False)

#files.download('heart_failure_processed.csv')

preprocessed_data_path = '/content/heart_failure_processed.csv'
df = pd.read_csv(preprocessed_data_path)

data_combined = df.copy()

features_to_log_transform = ['creatinine_phosphokinase', 'platelets', 'serum_creatinine']

for feature in features_to_log_transform:
    data_combined[feature] = np.log1p(data_combined[feature])

for feature in features_to_log_transform:
    Q1 = data_combined[feature].quantile(0.25)
    Q3 = data_combined[feature].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    data_combined = data_combined[(data_combined[feature] >= lower_bound) & (data_combined[feature] <= upper_bound)]

print("\nSummary statistics after combination of log transformation and outlier removal:")
print(data_combined.describe())

print("\nDataset shape after combination of log transformation and outlier removal:", data_combined.shape)

data_standard_scaled = data_combined.copy()

numerical_features = ['age', 'creatinine_phosphokinase', 'ejection_fraction',
                      'platelets', 'serum_creatinine', 'serum_sodium', 'time']

scaler_standard = StandardScaler()
data_standard_scaled[numerical_features] = scaler_standard.fit_transform(data_standard_scaled[numerical_features])

print("\nData after applying StandardScaler:")
print(data_standard_scaled.head())

print("\nSummary statistics after applying StandardScaler:")
print(data_standard_scaled[numerical_features].describe())

X = data_standard_scaled.drop(columns=['DEATH_EVENT'])
y = data_standard_scaled['DEATH_EVENT']

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

print("\nDataset split completed:")
print(f"Training Set: X_train = {X_train.shape}, y_train = {y_train.shape}")
print(f"Validation Set: X_val = {X_val.shape}, y_val = {y_val.shape}")
print(f"Test Set: X_test = {X_test.shape}, y_test = {y_test.shape}")

X_train_np = X_train.to_numpy()
X_val_np = X_val.to_numpy()
X_test_np = X_test.to_numpy()

y_train_np = y_train.to_numpy()
y_val_np = y_val.to_numpy()
y_test_np = y_test.to_numpy()

print(f"Before SMOTE: {Counter(y_train_np)}")
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_np, y_train_np = smote.fit_resample(X_train_np, y_train_np)
print(f"After SMOTE: {Counter(y_train_np)}")

X_train_seq = X_train_np[:, -1].reshape((X_train_np.shape[0], 1, 1))
X_val_seq = X_val_np[:, -1].reshape((X_val_np.shape[0], 1, 1))
X_test_seq = X_test_np[:, -1].reshape((X_test_np.shape[0], 1, 1))

X_train_np = X_train_np[:, :-1]
X_val_np = X_val_np[:, :-1]
X_test_np = X_test_np[:, :-1]

print("Data prepared for Option 2:")
print(f"Shape of X_train for static features: {X_train_np.shape}")
print(f"Shape of X_train_seq for sequential input (time): {X_train_seq.shape}")
print(f"Shape of y_train: {y_train_np.shape}")

static_input = Input(shape=(X_train_np.shape[1],), name='Static_Numerical_Input')

sequential_input = Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]), name='Sequential_Time_Input')

lstm_output = LSTM(16, activation='tanh', return_sequences=False)(sequential_input)

combined_input = Concatenate()([static_input, lstm_output])

dense_1 = Dense(64, activation='relu')(combined_input)
dropout_1 = Dropout(0.3)(dense_1)

dense_2 = Dense(32, activation='relu')(dropout_1)
dropout_2 = Dropout(0.3)(dense_2)

dense_3 = Dense(16, activation='relu')(dropout_2)
output_layer = Dense(1, activation='sigmoid', name='Output')(dense_3)

aminn_model_seq = Model(inputs=[static_input, sequential_input], outputs=output_layer, name='AMINN_Model_Sequential')

aminn_model_seq.summary()

aminn_model_seq.compile(optimizer='adam',
                        loss='binary_crossentropy',
                        metrics=['accuracy', tf.keras.metrics.AUC()])

history_lstm_smote = aminn_model_seq.fit([X_train_np, X_train_seq], y_train_np,
                                         validation_data=([X_val_np, X_val_seq], y_val_np),
                                         epochs=100,  # Adjust epochs as needed
                                         batch_size=32,  # Adjust batch size if necessary
                                         verbose=1)

test_loss_smote, test_accuracy_smote, test_auc_smote = aminn_model_seq.evaluate([X_test_np, X_test_seq], y_test_np, verbose=1)

print("\nEvaluation after SMOTE:")
print(f"Test Loss: {test_loss_smote}")
print(f"Test Accuracy: {test_accuracy_smote}")
print(f"Test AUC: {test_auc_smote}")

# Extracting the accuracy and loss for training
train_accuracy_seq = history_lstm_smote.history['accuracy']
train_loss_seq = history_lstm_smote.history['loss']

# Number of epochs
epochs_seq = range(1, len(train_accuracy_seq) + 1)

# Evaluate the model on the test set
test_loss_seq, test_accuracy_seq, _ = aminn_model_seq.evaluate([X_test_np, X_test_seq], y_test_np, verbose=0)

# Plot Training Accuracy and Test Accuracy
plt.figure(figsize=(18, 5))

plt.subplot(1, 2, 1)
plt.plot(epochs_seq, train_accuracy_seq, label='Training Accuracy', marker='o')
plt.axhline(y=test_accuracy_seq, color='r', linestyle='--', label=f'Test Accuracy: {test_accuracy_seq:.4f}')
plt.xlabel('Epochs', fontweight="bold")
plt.ylabel('Accuracy', fontweight="bold")
#plt.title('Training and Test Accuracy', fontweight="bold")
plt.legend()

# Plot Training Loss and Test Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_seq, train_loss_seq, label='Training Loss', marker='o')
plt.axhline(y=test_loss_seq, color='r', linestyle='--', label=f'Test Loss: {test_loss_seq:.4f}')
plt.xlabel('Epochs', fontweight="bold")
plt.ylabel('Loss', fontweight="bold")
#plt.title('Training and Test Loss', fontweight="bold")
plt.legend()

plt.tight_layout()
plt.show()

history_dict_seq = history_lstm_smote.history

train_accuracy_seq = history_dict_seq['accuracy']
val_accuracy_seq = history_dict_seq['val_accuracy']
train_loss_seq = history_dict_seq['loss']
val_loss_seq = history_dict_seq['val_loss']
train_auc_seq = history_dict_seq['auc']
val_auc_seq = history_dict_seq['val_auc']

epochs_seq = range(1, len(train_accuracy_seq) + 1)

plt.figure(figsize=(18, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs_seq, train_accuracy_seq, label='Training Accuracy', marker='o')
plt.plot(epochs_seq, val_accuracy_seq, label='Validation Accuracy', marker='o')
plt.xlabel('Epochs', fontweight="bold")
plt.ylabel('Accuracy', fontweight="bold")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs_seq, train_loss_seq, label='Training Loss', marker='o')
plt.plot(epochs_seq, val_loss_seq, label='Validation Loss', marker='o')
plt.xlabel('Epochs', fontweight="bold")
plt.ylabel('Loss', fontweight="bold")
plt.legend()

plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 5))
plt.plot(epochs_seq, train_auc_seq, label='Training AUC', marker='o')
plt.plot(epochs_seq, val_auc_seq, label='Validation AUC', marker='o')
plt.xlabel('Epochs', fontweight="bold")
plt.ylabel('Area under the curve', fontweight="bold")
plt.legend()
plt.show()

y_pred_prob_seq = aminn_model_seq.predict([X_test_np, X_test_seq])
y_pred_seq = (y_pred_prob_seq > 0.5).astype(int)

conf_matrix_seq = confusion_matrix(y_test_np, y_pred_seq)

disp_seq = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_seq, display_labels=["No Heart Failure", "Heart Failure"])
disp_seq.plot(cmap=plt.cm.Blues)
plt.show()
# -*- coding: utf-8 -*-
"""6.Model Testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s5B8IE0FaEisu8BHPLQmwFldFR6SGcnV
"""

import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import time

X_test = pd.read_csv('/content/X_test_processed.csv').values
y_test = pd.read_csv('/content/y_test_processed.csv').values
y_test = y_test.astype(np.float32)

print(X_test.shape)
print(y_test.shape)

class FeatureAttentionLayer(tf.keras.layers.Layer):
    def __init__(self, attention_transform_units=None, **kwargs):
        super(FeatureAttentionLayer, self).__init__(**kwargs)
        self.attention_transform_units = attention_transform_units

    def build(self, input_shape):
        if self.attention_transform_units is None:
            self.attention_transform_units = input_shape[-1]

        self.attention_transform_layer = tf.keras.layers.Dense(self.attention_transform_units, activation='tanh', use_bias=True, name='attention_transform')
        self.attention_weights_layer = tf.keras.layers.Dense(input_shape[-1], activation='softmax', use_bias=False, name='attention_weights')
        super(FeatureAttentionLayer, self).build(input_shape)

    def call(self, inputs):
        transformed_inputs = self.attention_transform_layer(inputs)
        attention_weights = self.attention_weights_layer(transformed_inputs)
        weighted_inputs = tf.keras.layers.Multiply()([inputs, attention_weights])
        return weighted_inputs

    def get_config(self):
        config = super(FeatureAttentionLayer, self).get_config()
        config.update({'attention_transform_units': self.attention_transform_units})
        return config

model_save_path_h5 = '/content/heart_failure_model.h5'

try:
    loaded_model = tf.keras.models.load_model(
        model_save_path_h5,
        custom_objects={'FeatureAttentionLayer': FeatureAttentionLayer}
    )
    print(f"Model '{model_save_path_h5}' loaded successfully!")
    loaded_model.summary()
except Exception as e:
    print(f"Error loading model: {e}")
    print("Please ensure the .h5 file exists at the specified path and the Custom Layer is correctly defined.")
    exit()

print("\n--- Evaluation using Loaded Model on Test Set ---")
test_loss, test_accuracy, test_auc = loaded_model.evaluate(X_test, y_test, verbose=0)

y_pred_proba = loaded_model.predict(X_test).ravel()

y_pred_binary = (y_pred_proba > 0.5).astype(int)

precision = precision_score(y_test, y_pred_binary)
recall = recall_score(y_test, y_pred_binary)
f1 = f1_score(y_test, y_pred_binary)

print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test Loss: {test_loss:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"AUC: {test_auc:.4f}")

print("\n--- Model Inference Time (Test Set) using Loaded Model ---")
start_time_warmup = time.time()
_ = loaded_model.predict(X_test)
end_time_warmup = time.time()
print(f"Warm-up prediction time for {X_test.shape[0]} samples: {(end_time_warmup - start_time_warmup):.4f} seconds")

start_time = time.time()
_ = loaded_model.predict(X_test)
end_time = time.time()
inference_time = end_time - start_time
print(f"Actual prediction time for {X_test.shape[0]} samples: {inference_time:.4f} seconds")
print(f"Average inference time per sample: {(inference_time / X_test.shape[0] * 1000):.4f} ms/sample")

cm = confusion_matrix(y_test, y_pred_binary)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()
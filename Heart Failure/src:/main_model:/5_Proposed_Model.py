# -*- coding: utf-8 -*-
"""5.Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UhWEauUyfxa9Du9B26vmJ1LhgSKJW2UI
"""

!pip install optuna-integration[tfkeras]

!pip install optuna

import time
import optuna
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow.keras.models import Model
from tensorflow.keras import backend as K
from tensorflow.keras import regularizers
from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Nadam, AdamW
from tensorflow.keras.layers import Input, Dense, Dropout, Multiply, Layer
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc

X_train = pd.read_csv('/content/X_train_final.csv').values
y_train = pd.read_csv('/content/y_train_final.csv').values

X_val = pd.read_csv('/content/X_val_processed.csv').values
y_val = pd.read_csv('/content/y_val_processed.csv').values

X_test = pd.read_csv('/content/X_test_processed.csv').values
y_test = pd.read_csv('/content/y_test_processed.csv').values

y_train = y_train.astype(np.float32)
y_val = y_val.astype(np.float32)
y_test = y_test.astype(np.float32)

print(X_train.shape, y_train.shape)
print(X_val.shape, y_val.shape)
print(X_test.shape, y_test.shape)

print(f"Shape of X_train for static features: {X_train.shape}")
print(f"Shape of y_train: {y_train.shape}")

X_train_combined = np.concatenate((X_train, X_val), axis=0)
y_train_combined = np.concatenate((y_train, y_val), axis=0)

print(f"Shape of X_train_combined: {X_train_combined.shape}")
print(f"Shape of y_train_combined: {y_train_combined.shape}")

input_dim = X_train.shape[1]

class FeatureAttentionLayer(Layer):
    def __init__(self, attention_transform_units=None, **kwargs):
        super(FeatureAttentionLayer, self).__init__(**kwargs)
        self.attention_transform_units = attention_transform_units

    def build(self, input_shape):
        if self.attention_transform_units is None:
            self.attention_transform_units = input_shape[-1]

        self.attention_transform_layer = Dense(self.attention_transform_units, activation='tanh', use_bias=True, name='attention_transform')
        self.attention_weights_layer = Dense(input_shape[-1], activation='softmax', use_bias=False, name='attention_weights')
        super(FeatureAttentionLayer, self).build(input_shape)

    def call(self, inputs):
        transformed_inputs = self.attention_transform_layer(inputs)
        attention_weights = self.attention_weights_layer(transformed_inputs)
        weighted_inputs = Multiply()([inputs, attention_weights])
        return weighted_inputs

    def get_config(self):
        config = super(FeatureAttentionLayer, self).get_config()
        config.update({'attention_transform_units': self.attention_transform_units})
        return config

def build_attention_mlp_model(
    input_dim,
    initial_dense_units,
    num_dense_layers_after_attention,
    dense_units_after_attention_list,
    dropout_rate,
    learning_rate,
    attention_transform_units,
    attention_placement,
    use_l2_regularization,
    l2_reg_strength,
    use_l1_regularization,
    l1_reg_strength,
    optimizer_name,
    weight_decay_rate=0.0
):
    tf.keras.backend.clear_session()

    kernel_regularizer = None

    if use_l1_regularization and use_l2_regularization:
        kernel_regularizer = regularizers.L1L2(l1=l1_reg_strength, l2=l2_reg_strength)
    elif use_l1_regularization:
        kernel_regularizer = regularizers.l1(l1_reg_strength)
    elif use_l2_regularization:
        kernel_regularizer = regularizers.l2(l2_reg_strength)

    if optimizer_name == 'adam':
        optimizer = Adam(learning_rate=learning_rate)
    elif optimizer_name == 'rmsprop':
        optimizer = RMSprop(learning_rate=learning_rate)
    elif optimizer_name == 'sgd':
        optimizer = SGD(learning_rate=learning_rate)
    elif optimizer_name == 'nadam':
        optimizer = Nadam(learning_rate=learning_rate)
    elif optimizer_name == 'adamw':
        optimizer = AdamW(learning_rate=learning_rate, weight_decay=weight_decay_rate)
    else:
        optimizer = Adam(learning_rate=learning_rate)


    combined_static_input = Input(shape=(input_dim,), name='Combined_Static_Input')
    x = combined_static_input

    if attention_placement == 'after_initial_dense':
        x = Dense(initial_dense_units, activation='relu', kernel_regularizer=kernel_regularizer, name='initial_dense')(x)
        x = Dropout(dropout_rate, name='initial_dropout')(x)
        x = FeatureAttentionLayer(attention_transform_units=attention_transform_units, name='feature_attention_layer')(x)
    else:
        x = FeatureAttentionLayer(attention_transform_units=attention_transform_units, name='feature_attention_layer')(x)
        x = Dense(initial_dense_units, activation='relu', kernel_regularizer=kernel_regularizer, name='initial_dense_after_attention')(x)
        x = Dropout(dropout_rate, name='initial_dropout_after_attention')(x)

    current_output = x
    for i in range(num_dense_layers_after_attention):
        current_output = Dense(dense_units_after_attention_list[i], activation='relu',
                               kernel_regularizer=kernel_regularizer,
                               name=f'dense_L{i+1}')(current_output)
        current_output = Dropout(dropout_rate, name=f'dropout_L{i+1}')(current_output)

    output_layer = Dense(1, activation='sigmoid', name='Output')(current_output)

    model = Model(inputs=combined_static_input, outputs=output_layer, name='MLP_with_FeatureAttention')
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])
    return model

def objective(trial):
    tf.keras.backend.clear_session()

    initial_dense_units = trial.suggest_categorical('initial_dense_units', [16, 32, 64, 128])
    num_dense_layers_after_attention = trial.suggest_int('num_dense_layers_after_attention', 1, 4)

    dense_units_after_attention_list = []
    for i in range(num_dense_layers_after_attention):
        units = trial.suggest_categorical(f'dense_units_L{i+1}', [8, 16, 32, 64, 128])
        dense_units_after_attention_list.append(units)

    attention_placement = trial.suggest_categorical('attention_placement', ['after_input', 'after_initial_dense'])
    attention_transform_units = trial.suggest_categorical('attention_transform_units', [16, 32, 64, 128])

    learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-2, log=True)
    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32, 64, 128, 256])

    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'rmsprop', 'sgd', 'nadam', 'adamw'])

    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.05)
    use_l2_regularization = trial.suggest_categorical('use_l2_regularization', [True, False])
    l2_reg_strength = 0.0
    if use_l2_regularization:
        l2_reg_strength = trial.suggest_float('l2_reg_strength', 1e-5, 1e-2, log=True)

    use_l1_regularization = trial.suggest_categorical('use_l1_regularization', [True, False])
    l1_reg_strength = 0.0
    if use_l1_regularization:
        l1_reg_strength = trial.suggest_float('l1_reg_strength', 1e-6, 5e-3, log=True)

    weight_decay_rate = 0.0
    if optimizer_name == 'adamw':
        weight_decay_rate = trial.suggest_float('weight_decay_rate', 1e-5, 1e-2, log=True)

    model = build_attention_mlp_model(
        input_dim=input_dim,
        initial_dense_units=initial_dense_units,
        num_dense_layers_after_attention=num_dense_layers_after_attention,
        dense_units_after_attention_list=dense_units_after_attention_list,
        dropout_rate=dropout_rate,
        learning_rate=learning_rate,
        attention_transform_units=attention_transform_units,
        attention_placement=attention_placement,
        use_l2_regularization=use_l2_regularization,
        l2_reg_strength=l2_reg_strength,
        use_l1_regularization=use_l1_regularization,
        l1_reg_strength=l1_reg_strength,
        optimizer_name=optimizer_name,
        weight_decay_rate=weight_decay_rate
    )

    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=12,
            mode='min',
            restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=7,
            min_lr=1e-6,
            mode='min',
            verbose=0
        ),
        optuna.integration.TFKerasPruningCallback(trial, monitor='val_loss')
    ]

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=200,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=0
    )

    best_val_loss = min(history.history['val_loss'])
    return best_val_loss

print("--- Starting Optuna Optimization Study ---")
study = optuna.create_study(direction='minimize', study_name='HeartFailureNN_Optimization',
                             pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=20))

study.optimize(objective, n_trials=200, show_progress_bar=True)

print("\n--- Optimization Finished ---")
print(f"Number of finished trials: {len(study.trials)}")
print(f"Best trial value (Validation Loss): {study.best_trial.value:.4f}")
print("Best trial hyperparameters:")
for key, value in study.best_trial.params.items():
    print(f"  {key}: {value}")

best_params = study.best_trial.params

best_dense_units_after_attention_list = []
for i in range(best_params['num_dense_layers_after_attention']):
    best_dense_units_after_attention_list.append(best_params[f'dense_units_L{i+1}'])

final_weight_decay_rate = best_params.get('weight_decay_rate', 0.0)
final_use_l1_regularization = best_params.get('use_l1_regularization', False)
final_l1_reg_strength = best_params.get('l1_reg_strength', 0.0)

final_model = build_attention_mlp_model(
    input_dim=input_dim,
    initial_dense_units=best_params['initial_dense_units'],
    num_dense_layers_after_attention=best_params['num_dense_layers_after_attention'],
    dense_units_after_attention_list=best_dense_units_after_attention_list,
    dropout_rate=best_params['dropout_rate'],
    learning_rate=best_params['learning_rate'],
    attention_transform_units=best_params['attention_transform_units'],
    attention_placement=best_params['attention_placement'],
    use_l2_regularization=best_params['use_l2_regularization'],
    l2_reg_strength=best_params.get('l2_reg_strength', 0.0),
    use_l1_regularization=final_use_l1_regularization,
    l1_reg_strength=final_l1_reg_strength,
    optimizer_name=best_params['optimizer'],
    weight_decay_rate=final_weight_decay_rate
)

final_callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        mode='min',
        restore_best_weights=True
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=10,
        min_lr=1e-6,
        mode='min'
    )
]

print("\nFinal Model Summary:")
final_model.summary()

final_history = final_model.fit(
    X_train_combined, y_train_combined,
    validation_split=0.1,
    epochs=200,
    batch_size=best_params['batch_size'],
    callbacks=final_callbacks,
    verbose=1
)

model_save_path_h5 = 'heart_failure_model.h5'
final_model.save(model_save_path_h5)
print(f"Model saved to: {model_save_path_h5}")

print("\n--- Final Model Evaluation on Test Set ---")
test_loss, test_accuracy, test_auc = final_model.evaluate(X_test, y_test, verbose=0)

y_pred_proba = final_model.predict(X_test).ravel()

y_pred_binary = (y_pred_proba > 0.5).astype(int)

precision = precision_score(y_test, y_pred_binary)
recall = recall_score(y_test, y_pred_binary)
f1 = f1_score(y_test, y_pred_binary)

print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test Loss: {test_loss:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"AUC: {test_auc:.4f}")

print("\n--- Model Inference Time (Test Set) ---")
start_time_warmup = time.time()
_ = final_model.predict(X_test)
end_time_warmup = time.time()
print(f"Warm-up prediction time for {X_test.shape[0]} samples: {(end_time_warmup - start_time_warmup):.4f} seconds")

start_time = time.time()
_ = final_model.predict(X_test)
end_time = time.time()
inference_time = end_time - start_time
print(f"Actual prediction time for {X_test.shape[0]} samples: {inference_time:.4f} seconds")
print(f"Average inference time per sample: {(inference_time / X_test.shape[0] * 1000):.4f} ms/sample")

history_dict = final_history.history
epochs_range = range(len(history_dict['accuracy']))

plt.figure(figsize=(10, 6))
plt.plot(epochs_range, history_dict['accuracy'], label='Training Accuracy')
plt.plot(epochs_range, history_dict['val_accuracy'], label='Validation Accuracy')
plt.plot(epochs_range, [test_accuracy] * len(epochs_range), label='Test Accuracy', linestyle='--')
#plt.title('Training, Validation, and Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(epochs_range, history_dict['loss'], label='Training Loss')
plt.plot(epochs_range, history_dict['val_loss'], label='Validation Loss')
plt.plot(epochs_range, [test_loss] * len(epochs_range), label='Test Loss', linestyle='--')
#plt.title('Training, Validation, and Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

cm = confusion_matrix(y_test, y_pred_binary)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
#plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
#plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()
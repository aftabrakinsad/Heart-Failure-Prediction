# -*- coding: utf-8 -*-
"""MLP_Statistical_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sT6jzcx2tOqAU5z5UM85bZjlUWYuC6Xz
"""

!pip install optuna

import optuna
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import KFold
from tensorflow.keras.models import Model
from tensorflow.keras import backend as K
from tensorflow.keras import regularizers
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Nadam, AdamW
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

X_train = pd.read_csv('/content/X_train_final.csv').values
y_train = pd.read_csv('/content/y_train_final.csv').values.astype(np.float32).flatten()

X_val = pd.read_csv('/content/X_val_processed.csv').values
y_val = pd.read_csv('/content/y_val_processed.csv').values.astype(np.float32).flatten()

X_combined_for_cv = np.concatenate((X_train, X_val), axis=0)
y_combined_for_cv = np.concatenate((y_train, y_val), axis=0)

input_dim = X_train.shape[1]

def build_mlp_baseline_model(
    input_dim,
    num_dense_layers,
    dense_units_list,
    dropout_rate,
    learning_rate,
    use_l2_regularization,
    l2_reg_strength,
    use_l1_regularization,
    l1_reg_strength,
    optimizer_name,
    weight_decay_rate=0.0
):
    tf.keras.backend.clear_session()
    kernel_regularizer = None
    if use_l1_regularization and use_l2_regularization:
        kernel_regularizer = regularizers.L1L2(l1=l1_reg_strength, l2=l2_reg_strength)
    elif use_l1_regularization:
        kernel_regularizer = regularizers.l1(l1_reg_strength)
    elif use_l2_regularization:
        kernel_regularizer = regularizers.l2(l2_reg_strength)

    if optimizer_name == 'adam':
        optimizer = Adam(learning_rate=learning_rate)
    elif optimizer_name == 'rmsprop':
        optimizer = RMSprop(learning_rate=learning_rate)
    elif optimizer_name == 'sgd':
        optimizer = SGD(learning_rate=learning_rate)
    elif optimizer_name == 'nadam':
        optimizer = Nadam(learning_rate=learning_rate)
    elif optimizer_name == 'adamw':
        optimizer = AdamW(learning_rate=learning_rate, weight_decay=weight_decay_rate)
    else:
        optimizer = Adam(learning_rate=learning_rate)

    main_input = Input(shape=(input_dim,), name='Main_Input')
    x = main_input

    for i in range(num_dense_layers):
        x = Dense(dense_units_list[i], activation='relu', kernel_regularizer=kernel_regularizer, name=f'dense_L{i+1}')(x)
        x = Dropout(dropout_rate, name=f'dropout_L{i+1}')(x)

    output_layer = Dense(1, activation='sigmoid', name='Output')(x)

    model = Model(inputs=main_input, outputs=output_layer, name='MLP_Baseline')
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])
    return model

def objective_mlp(trial):
    num_dense_layers = trial.suggest_int('num_dense_layers', 1, 4)
    dense_units_list = []
    for i in range(num_dense_layers):
        units = trial.suggest_categorical(f'dense_units_L{i+1}', [16, 32, 64, 128])
        dense_units_list.append(units)

    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.05)
    learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-2, log=True)
    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32, 64, 128, 256])
    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'rmsprop', 'sgd', 'nadam', 'adamw'])

    use_l2_regularization = trial.suggest_categorical('use_l2_regularization', [True, False])
    l2_reg_strength = 0.0
    if use_l2_regularization:
        l2_reg_strength = trial.suggest_float('l2_reg_strength', 1e-5, 1e-2, log=True)

    use_l1_regularization = trial.suggest_categorical('use_l1_regularization', [True, False])
    l1_reg_strength = 0.0
    if use_l1_regularization:
        l1_reg_strength = trial.suggest_float('l1_reg_strength', 1e-6, 5e-3, log=True)

    weight_decay_rate = 0.0
    if optimizer_name == 'adamw':
        weight_decay_rate = trial.suggest_float('weight_decay_rate', 1e-5, 1e-2, log=True)

    model = build_mlp_baseline_model(
        input_dim=input_dim,
        num_dense_layers=num_dense_layers,
        dense_units_list=dense_units_list,
        dropout_rate=dropout_rate,
        learning_rate=learning_rate,
        use_l2_regularization=use_l2_regularization,
        l2_reg_strength=l2_reg_strength,
        use_l1_regularization=use_l1_regularization,
        l1_reg_strength=l1_reg_strength,
        optimizer_name=optimizer_name,
        weight_decay_rate=weight_decay_rate
    )

    callbacks = [
        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=12, mode='min', restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6, mode='min', verbose=0)
    ]

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=200,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=0
    )

    best_val_loss = min(history.history['val_loss'])
    return best_val_loss

print("\n--- Starting Optuna Optimization Study for MLP Baseline (Minimizing Validation Loss) ---")
study_mlp = optuna.create_study(direction='minimize', study_name='MLP_Optimization',
                                pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=20))
study_mlp.optimize(objective_mlp, n_trials=200, show_progress_bar=True)

print("\n--- MLP Optimization Finished ---")
print(f"Number of finished trials: {len(study_mlp.trials)}")
print(f"Best trial value (Validation Loss): {study_mlp.best_trial.value:.4f}")
print("Best MLP hyperparameters:")
best_params_mlp = study_mlp.best_trial.params
for key, value in best_params_mlp.items():
    print(f"  {key}: {value}")

best_dense_units_list_mlp = []
for i in range(best_params_mlp['num_dense_layers']):
    best_dense_units_list_mlp.append(best_params_mlp[f'dense_units_L{i+1}'])

n_splits = 10
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

mlp_accuracy_scores = []
mlp_auc_scores = []
mlp_precision_scores = []
mlp_recall_scores = []
mlp_f1_scores = []

print(f"\n--- Starting {n_splits}-Fold Cross-Validation for Optimized MLP Baseline ---")

for fold, (train_index, val_index) in enumerate(kf.split(X_combined_for_cv, y_combined_for_cv)):
    print(f"\n--- Fold {fold+1}/{n_splits} ---")
    X_train_fold, X_val_fold = X_combined_for_cv[train_index], X_combined_for_cv[val_index]
    y_train_fold, y_val_fold = y_combined_for_cv[train_index], y_combined_for_cv[val_index]

    single_class_in_val_fold = (len(np.unique(y_val_fold)) <= 1)
    if single_class_in_val_fold:
        print(f"WARNING: Fold {fold+1} validation set contains only one class. AUC will be NaN.")

    mlp_model_cv = build_mlp_baseline_model(
        input_dim=input_dim,
        num_dense_layers=best_params_mlp['num_dense_layers'],
        dense_units_list=best_dense_units_list_mlp,
        dropout_rate=best_params_mlp['dropout_rate'],
        learning_rate=best_params_mlp['learning_rate'],
        use_l2_regularization=best_params_mlp['use_l2_regularization'],
        l2_reg_strength=best_params_mlp.get('l2_reg_strength', 0.0),
        use_l1_regularization=best_params_mlp['use_l1_regularization'],
        l1_reg_strength=best_params_mlp.get('l1_reg_strength', 0.0),
        optimizer_name=best_params_mlp['optimizer'],
        weight_decay_rate=best_params_mlp.get('weight_decay_rate', 0.0)
    )

    callbacks_cv = [
        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, mode='min', verbose=0)
    ]

    history_cv = mlp_model_cv.fit(
        X_train_fold, y_train_fold,
        validation_data=(X_val_fold, y_val_fold),
        epochs=200,
        batch_size=best_params_mlp['batch_size'],
        callbacks=callbacks_cv,
        verbose=0
    )

    y_pred_proba_mlp = mlp_model_cv.predict(X_val_fold).flatten()
    y_pred_class_mlp = (y_pred_proba_mlp > 0.5).astype(int)

    mlp_accuracy_scores.append(accuracy_score(y_val_fold, y_pred_class_mlp))
    if not single_class_in_val_fold:
        mlp_auc_scores.append(roc_auc_score(y_val_fold, y_pred_proba_mlp))
    else:
        mlp_auc_scores.append(np.nan)
    mlp_precision_scores.append(precision_score(y_val_fold, y_pred_class_mlp, zero_division=0))
    mlp_recall_scores.append(recall_score(y_val_fold, y_pred_class_mlp, zero_division=0))
    mlp_f1_scores.append(f1_score(y_val_fold, y_pred_class_mlp, zero_division=0))

    print(f"  MLP Baseline Fold {fold+1} Metrics:")
    print(f"    Accuracy: {mlp_accuracy_scores[-1]:.4f}")
    print(f"    AUC: {mlp_auc_scores[-1]:.4f}" if not np.isnan(mlp_auc_scores[-1]) else "AUC: N/A")
    print(f"    Precision: {mlp_precision_scores[-1]:.4f}")
    print(f"    Recall: {mlp_recall_scores[-1]:.4f}")
    print(f"    F1-Score: {mlp_f1_scores[-1]:.4f}")

print("--- Optimized MLP Baseline: 10-Fold Cross-Validation Results (Mean ± Standard Deviation) ---")
print(f"Accuracy: {np.nanmean(mlp_accuracy_scores):.4f} ± {np.nanstd(mlp_accuracy_scores):.4f}")
print(f"AUC: {np.nanmean(mlp_auc_scores):.4f} ± {np.nanstd(mlp_auc_scores):.4f}")
print(f"Precision: {np.nanmean(mlp_precision_scores):.4f} ± {np.nanstd(mlp_precision_scores):.4f}")
print(f"Recall: {np.nanmean(mlp_recall_scores):.4f} ± {np.nanstd(mlp_recall_scores):.4f}")
print(f"F1-Score: {np.nanmean(mlp_f1_scores):.4f} ± {np.nanstd(mlp_f1_scores):.4f}")

def bootstrap_confidence_interval(data, alpha=0.95, n_bootstraps=10000):
    if len(data) == 0 or np.all(np.isnan(data)):
        return (np.nan, np.nan)

    data = np.array(data)[~np.isnan(data)]
    if len(data) == 0:
        return (np.nan, np.nan)

    bootstrap_means = []
    for _ in range(n_bootstraps):
        sample = np.random.choice(data, size=len(data), replace=True)
        bootstrap_means.append(np.mean(sample))

    lower_percentile = (1 - alpha) / 2 * 100
    upper_percentile = (1 + alpha) / 2 * 100

    lower_bound = np.percentile(bootstrap_means, lower_percentile)
    upper_bound = np.percentile(bootstrap_means, upper_percentile)

    return (lower_bound, upper_bound)

print("\n--- Optimized MLP Baseline: 95% Confidence Intervals (from 10-Fold CV) ---")
ci_mlp_accuracy = bootstrap_confidence_interval(mlp_accuracy_scores)
ci_mlp_auc = bootstrap_confidence_interval(mlp_auc_scores)
ci_mlp_precision = bootstrap_confidence_interval(mlp_precision_scores)
ci_mlp_recall = bootstrap_confidence_interval(mlp_recall_scores)
ci_mlp_f1 = bootstrap_confidence_interval(mlp_f1_scores)

print(f"Accuracy CI: ({ci_mlp_accuracy[0]:.4f}, {ci_mlp_accuracy[1]:.4f})")
print(f"AUC CI: ({ci_mlp_auc[0]:.4f}, {ci_mlp_auc[1]:.4f})")
print(f"Precision CI: ({ci_mlp_precision[0]:.4f}, {ci_mlp_precision[1]:.4f})")
print(f"Recall CI: ({ci_mlp_recall[0]:.4f}, {ci_mlp_recall[1]:.4f})")
print(f"F1-Score CI: ({ci_mlp_f1[0]:.4f}, {ci_mlp_f1[1]:.4f})")
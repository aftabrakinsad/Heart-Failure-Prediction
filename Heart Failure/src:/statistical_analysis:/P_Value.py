# -*- coding: utf-8 -*-
"""P-Values.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12QqbbENTFyWD3TopQPBTXcO0-N3B34pe
"""

import numpy as np
import pandas as pd
from scipy.stats import wilcoxon

proposed_accuracy = np.array([0.9453, 0.9844, 0.9531, 0.8828, 0.9141, 0.9141, 0.9062, 0.9531, 0.9297, 0.9141])
proposed_auc = np.array([0.9919, 0.9978, 0.9924, 0.9631, 0.9626, 0.9515, 0.9753, 0.9924, 0.9841, 0.9661])
proposed_precision = np.array([0.9492, 0.9661, 0.9394, 0.8704, 0.8667, 0.8769, 0.9194, 0.9091, 0.8841, 0.8841])
proposed_recall = np.array([0.9333, 1.0000, 0.9688, 0.8545, 0.9455, 0.9500, 0.8906, 1.0000, 0.9839, 0.9531])
proposed_f1 = np.array([0.9412, 0.9828, 0.9538, 0.8624, 0.9043, 0.9120, 0.9048, 0.9524, 0.9313, 0.9173])

knn_accuracy = np.array([0.9121, 0.9121, 0.9341, 0.8778, 0.8889, 0.9111, 0.8667, 0.8889, 0.9333, 0.9000])
knn_auc = np.array([0.9248, 0.9252, 0.9738, 0.9686, 0.9141, 0.9446, 0.9228, 0.9573, 0.9658, 0.9248])
knn_precision = np.array([0.9333, 0.8947, 0.8235, 0.8750, 0.8182, 0.9048, 0.7778, 0.7500, 0.8462, 1.0000])
knn_recall = np.array([0.6667, 0.7391, 0.8235, 0.7241, 0.7500, 0.7600, 0.6364, 0.5625, 0.7333, 0.7188])
knn_f1 = np.array([0.7778, 0.8095, 0.8235, 0.7925, 0.7826, 0.8261, 0.7000, 0.6429, 0.7857, 0.8364])

lr_accuracy = np.array([0.8672, 0.8516, 0.7656, 0.7812, 0.8359, 0.7969, 0.8281, 0.8125, 0.8672, 0.7969])
lr_auc = np.array([0.9257, 0.9088, 0.8518, 0.8780, 0.8872, 0.8498, 0.8879, 0.8907, 0.9433, 0.8657])
lr_precision = np.array([0.8116, 0.8276, 0.8400, 0.7288, 0.7742, 0.8036, 0.8182, 0.8214, 0.8462, 0.7879])
lr_recall = np.array([0.9333, 0.8421, 0.6562, 0.7818, 0.8727, 0.7500, 0.8438, 0.7667, 0.8871, 0.8125])
lr_f1 = np.array([0.8682, 0.8348, 0.7368, 0.7544, 0.8205, 0.7759, 0.8308, 0.7931, 0.8661, 0.8000])

svm_accuracy = np.array([0.9531, 0.9297, 0.9766, 0.8906, 0.9297, 0.8750, 0.9141, 0.9297, 0.8672, 0.8906])
svm_auc = np.array([0.9752, 0.9681, 0.9695, 0.9599, 0.9741, 0.9554, 0.9651, 0.9804, 0.9311, 0.9661])
svm_precision = np.array([0.9219, 0.9000, 0.9552, 0.8596, 0.8966, 0.8667, 0.8732, 0.9048, 0.8947, 0.9032])
svm_recall = np.array([0.9833, 0.9474, 1.0000, 0.8909, 0.9455, 0.8667, 0.9688, 0.9500, 0.8226, 0.8750])
svm_f1 = np.array([0.9516, 0.9231, 0.9771, 0.8750, 0.9204, 0.8667, 0.9185, 0.9268, 0.8571, 0.8889])

rf_accuracy = np.array([0.9670, 0.9670, 0.9670, 0.9333, 0.9333, 0.9000, 0.9889, 0.9667, 0.9556, 0.9000])
rf_auc = np.array([0.9939, 0.9597, 0.9936, 0.9890, 0.9823, 0.9655, 0.9953, 0.9966, 0.9973, 0.9752])
rf_precision = np.array([0.9500, 1.0000, 0.9375, 0.9600, 1.0000, 0.8636, 1.0000, 1.0000, 0.9231, 0.9259])
rf_recall = np.array([0.9048, 0.8696, 0.8824, 0.8276, 0.7500, 0.7600, 0.9545, 0.8125, 0.8000, 0.7812])
rf_f1 = np.array([0.9268, 0.9302, 0.9091, 0.8889, 0.8571, 0.8085, 0.9767, 0.8966, 0.8571, 0.8475])

mlp_accuracy = np.array([0.9451, 0.8901, 0.9780, 0.9000, 0.9444, 0.8444, 0.9111, 0.9333, 0.9444, 0.8667])
mlp_auc = np.array([0.9646, 0.9182, 0.9952, 0.9426, 0.9665, 0.9003, 0.9592, 0.9527, 0.9849, 0.9267])
mlp_precision = np.array([0.8636, 0.8421, 0.9412, 0.8846, 1.0000, 0.7895, 0.9375, 0.9167, 0.9167, 0.8846])
mlp_recall = np.array([0.9048, 0.6957, 0.9412, 0.7931, 0.7917, 0.6000, 0.6818, 0.6875, 0.7333, 0.7188])
mlp_f1 = np.array([0.8837, 0.7619, 0.9412, 0.8364, 0.8837, 0.6818, 0.7895, 0.7857, 0.8148, 0.7931])

dt_accuracy = np.array([0.9560, 0.9451, 0.9341, 0.9444, 0.9000, 0.9000, 0.9667, 0.9556, 0.9444, 0.8778])
dt_auc = np.array([0.9680, 0.9348, 0.9777, 0.9678, 0.8684, 0.9348, 0.9512, 0.9329, 0.9773, 0.9246])
dt_precision = np.array([0.9048, 0.8750, 0.8667, 1.0000, 0.8947, 0.9000, 0.9524, 0.9286, 0.8125, 0.8621])
dt_recall = np.array([0.9048, 0.9130, 0.7647, 0.8276, 0.7083, 0.7200, 0.9091, 0.8125, 0.8667, 0.7812])
dt_f1 = np.array([0.9048, 0.8936, 0.8125, 0.9057, 0.7907, 0.8000, 0.9302, 0.8667, 0.8387, 0.8197])

metrics = ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1-Score']
baseline_models = {
    'KNN Baseline': {
        'Accuracy': knn_accuracy, 'AUC': knn_auc, 'Precision': knn_precision,
        'Recall': knn_recall, 'F1-Score': knn_f1
    },
    'Logistic Regression Baseline': {
        'Accuracy': lr_accuracy, 'AUC': lr_auc, 'Precision': lr_precision,
        'Recall': lr_recall, 'F1-Score': lr_f1
    },
    'SVM Baseline': {
        'Accuracy': svm_accuracy, 'AUC': svm_auc, 'Precision': svm_precision,
        'Recall': svm_recall, 'F1-Score': svm_f1
    },
    'Random Forest Baseline': {
        'Accuracy': rf_accuracy, 'AUC': rf_auc, 'Precision': rf_precision,
        'Recall': rf_recall, 'F1-Score': rf_f1
    },
    'MLP Baseline': {
        'Accuracy': mlp_accuracy, 'AUC': mlp_auc, 'Precision': mlp_precision,
        'Recall': mlp_recall, 'F1-Score': mlp_f1
    },
    'Decision Tree Baseline': {
        'Accuracy': dt_accuracy, 'AUC': dt_auc, 'Precision': dt_precision,
        'Recall': dt_recall, 'F1-Score': dt_f1
    }
}

proposed_model_scores = {
    'Accuracy': proposed_accuracy, 'AUC': proposed_auc, 'Precision': proposed_precision,
    'Recall': proposed_recall, 'F1-Score': proposed_f1
}

results = []

print("--- Statistical Comparison: Proposed Model vs. Baselines (Wilcoxon Signed-Rank Test) ---")
print("--- Null Hypothesis (H0): There is no significant difference in performance. ---")
print("--- Alternative Hypothesis (H1): Proposed Model's performance is significantly different from Baseline. ---")
print("--- Significance Level (alpha) = 0.05 (Unadjusted for Multiple Comparisons) ---")

for metric in metrics:
    print(f"\n--- Metric: {metric} ---")
    for baseline_name, baseline_scores in baseline_models.items():
        diff_scores = proposed_model_scores[metric] - baseline_scores[metric]

        if np.all(diff_scores == 0):
            p_val = 1.0
            stat = 0.0
            significance = "Not Significant"
            print(f"  Proposed vs {baseline_name}: No difference in scores. p-value = {p_val:.4f} (Exact)")
        else:
            try:
                stat, p_val = wilcoxon(proposed_model_scores[metric], baseline_scores[metric], alternative='two-sided', method='approx')
                significance = "Significant" if p_val < 0.05 else "Not Significant"
                print(f"  Proposed vs {baseline_name}: W-statistic = {stat:.2f}, p-value = {p_val:.4f} ({significance})")
            except ValueError as e:
                p_val = np.nan
                stat = np.nan
                significance = "N/A (Error)"
                print(f"  Proposed vs {baseline_name}: Could not compute Wilcoxon test ({e}). p-value = N/A")

        proposed_mean = np.nanmean(proposed_model_scores[metric])
        baseline_mean = np.nanmean(baseline_scores[metric])
        mean_diff = proposed_mean - baseline_mean

        results.append({
            'Metric': metric,
            'Comparison': f'Proposed vs {baseline_name}',
            'W_statistic': stat,
            'p_value': p_val,
            'Significance': significance,
            'Mean_Difference': mean_diff
        })

print("--- Summary of Statistical Significance (Unadjusted P-values) ---")
print("A 'Significant' status indicates p-value < 0.05 for that individual test.")
print("Caution: This approach does NOT control for the increased risk of false positives when performing multiple comparisons.")

summary_results_output = []
for metric in metrics:
    summary_results_output.append(f"\n--- Metric: {metric} ---")
    for res in results:
        if res['Metric'] == metric:
            sign_str = res['Significance']
            p_val_str = f"p-value = {res['p_value']:.4f}" if not np.isnan(res['p_value']) else "p-value = N/A"
            mean_diff_str = f" (Mean diff: {res['Mean_Difference']:.4f})"
            summary_results_output.append(
                f"{res['Comparison']}: {sign_str}, {p_val_str}{mean_diff_str}"
            )

for line in summary_results_output:
    print(line)
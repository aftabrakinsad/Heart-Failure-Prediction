# -*- coding: utf-8 -*-
"""Proposed Model_Statistical Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14TYjIbYu3ArRVhtSvXDCaNqdw3xcDF66
"""

import time
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras import backend as K
from tensorflow.keras import regularizers
from sklearn.model_selection import KFold
from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Nadam, AdamW
from tensorflow.keras.layers import Input, Dense, Dropout, Multiply, Layer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

X_train = pd.read_csv('/content/X_train_final.csv').values
y_train = pd.read_csv('/content/y_train_final.csv').values.astype(np.float32).flatten()

X_val = pd.read_csv('/content/X_val_processed.csv').values
y_val = pd.read_csv('/content/y_val_processed.csv').values.astype(np.float32).flatten()

X_combined_for_cv = np.concatenate((X_train, X_val), axis=0)
y_combined_for_cv = np.concatenate((y_train, y_val), axis=0)

print(f"Shape of combined data for CV: {X_combined_for_cv.shape}")
print(f"Shape of combined labels for CV: {y_combined_for_cv.shape}")

input_dim = X_train.shape[1]

class FeatureAttentionLayer(Layer):
    def __init__(self, attention_transform_units=None, **kwargs):
        super(FeatureAttentionLayer, self).__init__(**kwargs)
        self.attention_transform_units = attention_transform_units

    def build(self, input_shape):
        if self.attention_transform_units is None:
            self.attention_transform_units = input_shape[-1]

        self.attention_transform_layer = Dense(self.attention_transform_units, activation='tanh', use_bias=True, name='attention_transform')
        self.attention_weights_layer = Dense(input_shape[-1], activation='softmax', use_bias=False, name='attention_weights')
        super(FeatureAttentionLayer, self).build(input_shape)

    def call(self, inputs):
        transformed_inputs = self.attention_transform_layer(inputs)
        attention_weights = self.attention_weights_layer(transformed_inputs)
        weighted_inputs = Multiply()([inputs, attention_weights])
        return weighted_inputs

    def get_config(self):
        config = super(FeatureAttentionLayer, self).get_config()
        config.update({'attention_transform_units': self.attention_transform_units})
        return config

def build_proposed_model(
    input_dim,
    initial_dense_units,
    num_dense_layers_after_attention,
    dense_units_after_attention_list,
    dropout_rate,
    learning_rate,
    attention_transform_units,
    attention_placement,
    use_l2_regularization,
    l2_reg_strength,
    use_l1_regularization,
    l1_reg_strength,
    optimizer_name,
    weight_decay_rate=0.0
):
    tf.keras.backend.clear_session()

    kernel_regularizer = None
    if use_l1_regularization and use_l2_regularization:
        kernel_regularizer = regularizers.L1L2(l1=l1_reg_strength, l2=l2_reg_strength)
    elif use_l1_regularization:
        kernel_regularizer = regularizers.l1(l1_reg_strength)
    elif use_l2_regularization:
        kernel_regularizer = regularizers.l2(l2_reg_strength)

    if optimizer_name == 'adam':
        optimizer = Adam(learning_rate=learning_rate)
    elif optimizer_name == 'rmsprop':
        optimizer = RMSprop(learning_rate=learning_rate)
    elif optimizer_name == 'sgd':
        optimizer = SGD(learning_rate=learning_rate)
    elif optimizer_name == 'nadam':
        optimizer = Nadam(learning_rate=learning_rate)
    elif optimizer_name == 'adamw':
        optimizer = AdamW(learning_rate=learning_rate, weight_decay=weight_decay_rate)
    else:
        optimizer = Adam(learning_rate=learning_rate)

    combined_static_input = Input(shape=(input_dim,), name='Combined_Static_Input')
    x = combined_static_input

    if attention_placement == 'after_initial_dense':
        x = Dense(initial_dense_units, activation='relu', kernel_regularizer=kernel_regularizer, name='initial_dense')(x)
        x = Dropout(dropout_rate, name='initial_dropout')(x)
        x = FeatureAttentionLayer(attention_transform_units=attention_transform_units, name='feature_attention_layer')(x)
    else:
        x = FeatureAttentionLayer(attention_transform_units=attention_transform_units, name='feature_attention_layer')(x)
        x = Dense(initial_dense_units, activation='relu', kernel_regularizer=kernel_regularizer, name='initial_dense_after_attention')(x)
        x = Dropout(dropout_rate, name='initial_dropout_after_attention')(x)

    current_output = x
    for i in range(num_dense_layers_after_attention):
        current_output = Dense(dense_units_after_attention_list[i], activation='relu',
                               kernel_regularizer=kernel_regularizer,
                               name=f'dense_L{i+1}')(current_output)
        current_output = Dropout(dropout_rate, name=f'dropout_L{i+1}')(current_output)

    output_layer = Dense(1, activation='sigmoid', name='Output')(current_output)

    model = Model(inputs=combined_static_input, outputs=output_layer, name='MLP_with_FeatureAttention')
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])
    return model

best_params = {
    'initial_dense_units': 32,
    'num_dense_layers_after_attention': 3,
    'dense_units_L1': 64,
    'dense_units_L2': 64,
    'dense_units_L3': 16,
    'attention_placement': 'after_initial_dense',
    'attention_transform_units': 64,
    'learning_rate': 0.0073866678565409506,
    'batch_size': 32,
    'optimizer': 'rmsprop',
    'dropout_rate': 0.25,
    'use_l2_regularization': False,
    'l2_reg_strength': 0.0,
    'use_l1_regularization': False,
    'l1_reg_strength': 0.0,
    'weight_decay_rate': 0.0
}

best_dense_units_after_attention_list = [
    best_params[f'dense_units_L{i+1}'] for i in range(best_params['num_dense_layers_after_attention'])
]

n_splits = 10
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

accuracy_scores = []
auc_scores = []
precision_scores = []
recall_scores = []
f1_scores = []

print(f"\n--- Starting {n_splits}-Fold Cross-Validation for Proposed Model ---")

for fold, (train_index, val_index) in enumerate(kf.split(X_combined_for_cv, y_combined_for_cv)):
    print(f"\n--- Fold {fold+1}/{n_splits} ---")
    X_train_fold, X_val_fold = X_combined_for_cv[train_index], X_combined_for_cv[val_index]
    y_train_fold, y_val_fold = y_combined_for_cv[train_index], y_combined_for_cv[val_index]

    model_for_cv = build_proposed_model(
        input_dim=input_dim,
        initial_dense_units=best_params['initial_dense_units'],
        num_dense_layers_after_attention=best_params['num_dense_layers_after_attention'],
        dense_units_after_attention_list=best_dense_units_after_attention_list,
        dropout_rate=best_params['dropout_rate'],
        learning_rate=best_params['learning_rate'],
        attention_transform_units=best_params['attention_transform_units'],
        attention_placement=best_params['attention_placement'],
        use_l2_regularization=best_params['use_l2_regularization'],
        l2_reg_strength=best_params['l2_reg_strength'],
        use_l1_regularization=best_params['use_l1_regularization'],
        l1_reg_strength=best_params['l1_reg_strength'],
        optimizer_name=best_params['optimizer'],
        weight_decay_rate=best_params['weight_decay_rate']
    )

    callbacks_cv = [
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            mode='min',
            restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=10,
            min_lr=1e-6,
            mode='min',
            verbose=0
        )
    ]

    history_cv = model_for_cv.fit(
        X_train_fold, y_train_fold,
        validation_data=(X_val_fold, y_val_fold),
        epochs=200,
        batch_size=best_params['batch_size'],
        callbacks=callbacks_cv,
        verbose=0
    )

    y_pred_proba_fold = model_for_cv.predict(X_val_fold).flatten()
    y_pred_class_fold = (y_pred_proba_fold > 0.5).astype(int)

    accuracy_scores.append(accuracy_score(y_val_fold, y_pred_class_fold))

    if len(np.unique(y_val_fold)) > 1:
        auc_scores.append(roc_auc_score(y_val_fold, y_pred_proba_fold))
    else:
        auc_scores.append(np.nan)

    precision_scores.append(precision_score(y_val_fold, y_pred_class_fold, zero_division=0))
    recall_scores.append(recall_score(y_val_fold, y_pred_class_fold, zero_division=0))
    f1_scores.append(f1_score(y_val_fold, y_pred_class_fold, zero_division=0))

    print(f"  Fold {fold+1} Metrics:")
    print(f"    Accuracy: {accuracy_scores[-1]:.4f}")
    print(f"    AUC: {auc_scores[-1]:.4f}" if not np.isnan(auc_scores[-1]) else "AUC: N/A (single class in fold)")
    print(f"    Precision: {precision_scores[-1]:.4f}")
    print(f"    Recall: {recall_scores[-1]:.4f}")
    print(f"    F1-Score: {f1_scores[-1]:.4f}")

print("\n--- Proposed Model: 10-Fold Cross-Validation Results (Mean ± Standard Deviation) ---")
print(f"Accuracy: {np.nanmean(accuracy_scores):.4f} ± {np.nanstd(accuracy_scores):.4f}")
print(f"AUC: {np.nanmean(auc_scores):.4f} ± {np.nanstd(auc_scores):.4f}")
print(f"Precision: {np.nanmean(precision_scores):.4f} ± {np.nanstd(precision_scores):.4f}")
print(f"Recall: {np.nanmean(recall_scores):.4f} ± {np.nanstd(recall_scores):.4f}")
print(f"F1-Score: {np.nanmean(f1_scores):.4f} ± {np.nanstd(f1_scores):.4f}")

def bootstrap_confidence_interval(data, alpha=0.95, n_bootstraps=10000):

    if len(data) == 0 or np.all(np.isnan(data)):
        return (np.nan, np.nan)

    data = np.array(data)[~np.isnan(data)]
    if len(data) == 0:
        return (np.nan, np.nan)

    bootstrap_means = []
    for _ in range(n_bootstraps):
        sample = np.random.choice(data, size=len(data), replace=True)
        bootstrap_means.append(np.mean(sample))

    lower_percentile = (1 - alpha) / 2 * 100
    upper_percentile = (1 + alpha) / 2 * 100

    lower_bound = np.percentile(bootstrap_means, lower_percentile)
    upper_bound = np.percentile(bootstrap_means, upper_percentile)

    return (lower_bound, upper_bound)

print("\n--- Proposed Model: 95% Confidence Intervals (from 10-Fold CV) ---")

ci_accuracy = bootstrap_confidence_interval(accuracy_scores)
ci_auc = bootstrap_confidence_interval(auc_scores)
ci_precision = bootstrap_confidence_interval(precision_scores)
ci_recall = bootstrap_confidence_interval(recall_scores)
ci_f1 = bootstrap_confidence_interval(f1_scores)

print(f"Accuracy CI: ({ci_accuracy[0]:.4f}, {ci_accuracy[1]:.4f})")
print(f"AUC CI: ({ci_auc[0]:.4f}, {ci_auc[1]:.4f})")
print(f"Precision CI: ({ci_precision[0]:.4f}, {ci_precision[1]:.4f})")
print(f"Recall CI: ({ci_recall[0]:.4f}, {ci_recall[1]:.4f})")
print(f"F1-Score CI: ({ci_f1[0]:.4f}, {ci_f1[1]:.4f})")